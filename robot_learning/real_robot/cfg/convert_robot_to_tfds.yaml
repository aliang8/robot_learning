hydra:
  searchpath:
    - pkg://robot_learning.cfg
    
env_name: robot
task_name: pick_up_bowl
seed: 0
data_dir: ${paths.data_dir}/datasets
tfds_data_dir: ${paths.data_dir}/tensorflow_datasets
dataset_name: robot_play
black_white: False
batch_size: 64
framestack: 1
save_imgs: False
debug: False
image_size: [128, 128]

compute_2d_flow: False
flow:
  # TODO: fix the paths
  sam_path: ${paths.sam_path}
  sam2_checkpoint: checkpoints/sam2.1_hiera_large.pt
  model_cfg: sam2.1_hiera_l.yaml
  model_id: IDEA-Research/grounding-dino-tiny

  # `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`
  video_dir: notebooks/videos/robot
  video_file: data/videos/2024-09-21_13-26-56_raw_traj_group0_traj0.mp4

  # setup the input image and text prompt for SAM 2 and Grounding DINO
      # VERY important: text queries need to be lowercased + end with a dot
  text: "robot. object." # TODO: this should change based on env
  device: cuda

  base_path: ${paths.optical_flow_path}
  cotracker_ckpt: scaled_offline.pth
  seg_mask_file: data/videos/segm_mask.npy
  grid_size: 25
  
  save_tracking_video: False
  tracking_dir: results
  tracking_filename: segm_grid
  max_query_points: 40

# Image embedding options
precompute_embeddings: false
embedding_model: 'r3m'  # Options: 'r3m', 'resnet50', 'radio-h', etc.

defaults:
  - local: default
  - _self_